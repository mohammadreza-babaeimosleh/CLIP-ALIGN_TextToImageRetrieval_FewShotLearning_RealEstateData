# -*- coding: utf-8 -*-
"""Text to image model test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RVqtGFesaB2TOBK6KCZtcSwp5FZA98J3
"""

!pip install openai
!pip install pydantic
!pip install ftfy regex tqdm
!pip install git+https://github.com/openai/CLIP.git
!pip install torch torchvision transformers pillow requests faiss-gpu loguru

import pandas as pd
import numpy as np
from datetime import datetime
import requests
import json
from pathlib import Path
import sys
from loguru import logger
from pprint import pprint
import re
import time
import os

import requests
import urllib

from pydantic import BaseModel, ValidationError, validator
from typing import Any, Dict, List, Optional

from google.colab import drive

import torch
from torch.utils.data import DataLoader, Dataset
from transformers import CLIPProcessor, CLIPModel
from PIL import Image
from tqdm import tqdm

import faiss
from transformers import AlignProcessor, AlignModel

# Step 1: Mount Google Drive
drive.mount('/content/drive', force_remount=True)

train_image_path = '/content/drive/My Drive/Building_images_train/'
train_label_path = '/content/drive/My Drive/Building_images_train/train_labels.json'

model_dir = '/content/drive/My Drive/Image_text models/CLIP_fine-tunned_batch1/'

test_image_path = '/content/drive/My Drive/Building_images_test_case/'
test_list_path = ["10130743",
                  "10130755",
                  "10130787",
                  "10130809",
                  "10130812",
                  "10130818",
                  "10130838",
                  "10130844",
                  "10130846",
                  "10130851"
                  ]
# model_dir = '/content/drive/My Drive/Image_text models/Clip_fine-tunned_conservative'
# model_dir = '/content/drive/My Drive/Image_text models/Clip_fine-tunned_conservative'

# Load the JSON file containing image labels
with open(train_label_path, 'r') as f:
    image_labels = json.load(f)

# Create a mapping from labels to images
label_to_images_train = {}
for image, labels in image_labels.items():
    for label in labels:
        label_to_images_train.setdefault(label, set()).add(image)

# Load the JSON file containing image labels
def load_test_labels(test_path):
    image_labels = {}
    with open(test_path, 'r') as f:
        image_labels = json.load(f)

    # Create a mapping from labels to images
    label_to_images = {}
    for image, labels in image_labels.items():
        for label in labels:
            label_to_images.setdefault(label, set()).add(image)

    # Get the set of all unique labels
    unique_labels = set(label_to_images.keys())

    return label_to_images, unique_labels

def search_image_finetuned(model, processor, query, image_dir, threshold=4, fix_threshold=0.23):
    """
    Args:
        query (str): The text query to search for.
        image_dir (str): Directory containing the images.
        threshold (float): The percentage drop threshold between consecutive scores.

    Returns:
        List of tuples (image_name, score) and a list of corresponding images.
    """
    model.eval()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    # Prepare the text input
    text_inputs = processor(text=[query], return_tensors="pt", padding=True)
    text_inputs = {k: v.to(device) for k, v in text_inputs.items()}

    # Get text embedding
    with torch.no_grad():
        text_features = model.get_text_features(**text_inputs)
        text_features /= text_features.norm(dim=-1, keepdim=True)  # Normalize

    # Compute similarity scores for all images
    image_scores = []
    images = {}

    for image_name in os.listdir(image_dir):
        if image_name.endswith(".json"):
            continue
        image_path = os.path.join(image_dir, image_name)
        image = Image.open(image_path).convert("RGB")

        # Process the image
        image_inputs = processor(images=image, return_tensors="pt")
        image_inputs = {k: v.to(device) for k, v in image_inputs.items()}

        # Get image embedding
        with torch.no_grad():
            image_features = model.get_image_features(**image_inputs)
            image_features /= image_features.norm(dim=-1, keepdim=True)  # Normalize

        # Compute cosine similarity
        similarity = torch.matmul(text_features, image_features.T).item()
        image_scores.append((image_name, similarity))
        images[image_name] = image

    # Sort by similarity score in descending order
    image_scores = sorted(image_scores, key=lambda x: x[1], reverse=True)

    # Select images based on score drop
    selected_scores = [image_scores[0]]  # Always include the first image
    for i in range(1, len(image_scores)):
        prev_score = image_scores[i - 1][1]
        curr_score = image_scores[i][1]
        percentage_drop = (prev_score - curr_score) / prev_score
        percentage_drop = percentage_drop * 100

        if percentage_drop > threshold:
            break
        elif curr_score > 0.22:
            selected_scores.append(image_scores[i])


    if image_scores[0][1] < fix_threshold:
        logger.info("All the images are bellow threshold")
        selected_scores = []

    # Get the corresponding images
    selected_images = [image_name for image_name, _ in selected_scores]

    return set(selected_images)

import pandas as pd

def test_result(unique_labels, label_to_images, model, processor, test_image_path, model_name, test_dataset_name, dynamic_threshold):
    total_tp = 0
    total_tn = 0
    total_images = 0
    label_accuracies = {}  # To store accuracies for each label
    all_test_images = set.union(*label_to_images.values())  # Union of all test images

    for label in unique_labels:
        # Ground truth: images that should be retrieved for this label
        true_images = label_to_images[label]

        train_flag = 1
        try:
            train_images = label_to_images_train[label]
        except:
            train_flag = 0

        # Retrieved images from the model
        retrieved_images = search_image_finetuned(
            model=model,
            processor=processor,
            query=label,
            image_dir=test_image_path,
            threshold=dynamic_threshold,
            fix_threshold=0.23
        )

        retrieved_images = set(retrieved_images)  # Ensure it's a set for set operations

        # True Positives (TP): Correctly retrieved images
        tp = true_images & retrieved_images

        # False Positives (FP): Images incorrectly retrieved
        fp = retrieved_images - true_images

        # False Negatives (FN): Images that should have been retrieved but were not
        fn = true_images - retrieved_images

        # True Negatives (TN): Images that are not in the label and not retrieved
        tn = (all_test_images - true_images) - (retrieved_images - true_images)

        # Accuracy calculation including TN
        label_total_images = len(tp) + len(fp) + len(fn) + len(tn)
        label_accuracy = (len(tp) + len(tn)) / label_total_images if label_total_images > 0 else 0

        # Accumulate for total accuracy calculation
        total_tp += len(tp)
        total_tn += len(tn)
        total_images += label_total_images

        # Store label accuracy
        label_accuracies[label] = label_accuracy

        # Print the results for this label
        print(f"Label: {label}")
        if train_flag:
            print(f"Image Count in Train: {len(train_images)}")
        else:
            print(f"Image Count in Train: 0")
        print(f"Image Count in Test: {len(true_images)}")
        print(f"Retrieved Image Count: {len(retrieved_images)}")
        print(f"Accuracy (with TN): {label_accuracy:.2f}")
        print(f"False Positives (FP): {list(fp)}")
        print(f"False Negatives (FN): {list(fn)}")
        print(f"True Negatives (TN): {len(tn)}")
        print("-" * 30)

    # Calculate total accuracy including TN
    total_accuracy = (total_tp + total_tn) / total_images if total_images > 0 else 0

    # Print total accuracy with color
    print("\033[92m" + f"Total Accuracy (with TN): {total_accuracy:.2f}" + "\033[0m")  # Green text

    # Prepare data for Excel
    row_data = {'Model Name': model_name, 'Test Dataset': test_dataset_name}
    row_data.update(label_accuracies)
    row_data['Total Accuracy'] = total_accuracy

    # Return the row data instead of writing to Excel
    return row_data

"""# CLIP fine-tuned"""

model_dir = '/content/drive/My Drive/Image_text models/CLIP_fine-tunned/'

# Initialize the list to collect all results
all_results = []

# Collect all unique labels across all datasets
all_labels = set()

for test_subdir in test_list_path:
    print(f"Testing {test_subdir}")
    model_name = model_dir.split('/')[-2]
    test_image_path = f'/content/drive/My Drive/Building_images_test_case/{test_subdir}'
    label_to_images, unique_labels = load_test_labels(f'{test_image_path}/test_labels.json')
    model = CLIPModel.from_pretrained(model_dir)
    processor = CLIPProcessor.from_pretrained(model_dir)

    # Call the modified test_result function and collect the results
    row_data = test_result(
        unique_labels=unique_labels,
        label_to_images=label_to_images,
        model=model,
        processor=processor,
        test_image_path=test_image_path,
        model_name=model_name,
        test_dataset_name=test_subdir,
        dynamic_threshold = 4
    )
    all_results.append(row_data)  # Add the row data to the list

    # Update the set of all labels
    all_labels.update(unique_labels)
    print("=" * 150)
    print("=" * 150)
    print("=" * 150)

# After processing all datasets, create a DataFrame
df = pd.DataFrame(all_results)

# Ensure all labels are included as columns in the DataFrame
for label in all_labels:
    if label not in df.columns:
        df[label] = None  # Set missing labels to None

# Reorder columns to have 'Model Name', 'Test Dataset', labels, and 'Total Accuracy'
columns_order = ['Model Name', 'Test Dataset'] + sorted(all_labels) + ['Total Accuracy']
df = df.reindex(columns=columns_order)

# Replace NaN values with 'None' as a string
df = df.fillna('None')

# Write the DataFrame to Excel
output_excel_path = f'{model_name}_all_datasets.xlsx'
df.to_excel(output_excel_path, index=False)
print(f"All results written to {output_excel_path}")

"""# Conservative Loss"""

def search_image_finetuned(model, processor, query, image_dir, threshold=4, fix_threshold=0.23):
    """
    Args:
        query (str): The text query to search for.
        image_dir (str): Directory containing the images.
        threshold (float): The percentage drop threshold between consecutive scores.

    Returns:
        List of tuples (image_name, score) and a list of corresponding images.
    """
    model.eval()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    # Prepare the text input
    text_inputs = processor(text=[query], return_tensors="pt", padding=True)
    text_inputs = {k: v.to(device) for k, v in text_inputs.items()}

    # Get text embedding
    with torch.no_grad():
        text_features = model.get_text_features(**text_inputs)
        text_features /= text_features.norm(dim=-1, keepdim=True)  # Normalize

    # Compute similarity scores for all images
    image_scores = []
    images = {}

    for image_name in os.listdir(image_dir):
        if image_name.endswith(".json"):
            continue
        image_path = os.path.join(image_dir, image_name)
        image = Image.open(image_path).convert("RGB")

        # Process the image
        image_inputs = processor(images=image, return_tensors="pt")
        image_inputs = {k: v.to(device) for k, v in image_inputs.items()}

        # Get image embedding
        with torch.no_grad():
            image_features = model.get_image_features(**image_inputs)
            image_features /= image_features.norm(dim=-1, keepdim=True)  # Normalize

        # Compute cosine similarity
        similarity = torch.matmul(text_features, image_features.T).item()
        image_scores.append((image_name, similarity))
        images[image_name] = image

    # Sort by similarity score in descending order
    image_scores = sorted(image_scores, key=lambda x: x[1], reverse=True)

    # Select images based on score drop
    selected_scores = [image_scores[0]]  # Always include the first image
    for i in range(1, len(image_scores)):
        prev_score = image_scores[i - 1][1]
        curr_score = image_scores[i][1]
        percentage_drop = (prev_score - curr_score) / prev_score
        percentage_drop = percentage_drop * 100

        if percentage_drop > threshold:
            break
        elif curr_score > 0.50:
            selected_scores.append(image_scores[i])


    if image_scores[0][1] < fix_threshold:
        logger.info("All the images are bellow threshold")
        selected_scores = []

    # Get the corresponding images
    selected_images = [image_name for image_name, _ in selected_scores]

    return set(selected_images)

import pandas as pd

def test_result(unique_labels, label_to_images, model, processor, test_image_path, model_name, test_dataset_name, dynamic_threshold):
    total_tp = 0
    total_tn = 0
    total_images = 0
    label_accuracies = {}  # To store accuracies for each label
    all_test_images = set.union(*label_to_images.values())  # Union of all test images

    for label in unique_labels:
        # Ground truth: images that should be retrieved for this label
        true_images = label_to_images[label]

        train_flag = 1
        try:
            train_images = label_to_images_train[label]
        except:
            train_flag = 0

        # Retrieved images from the model
        retrieved_images = search_image_finetuned(
            model=model,
            processor=processor,
            query=label,
            image_dir=test_image_path,
            threshold=dynamic_threshold,
            fix_threshold=0.5
        )

        retrieved_images = set(retrieved_images)  # Ensure it's a set for set operations

        # True Positives (TP): Correctly retrieved images
        tp = true_images & retrieved_images

        # False Positives (FP): Images incorrectly retrieved
        fp = retrieved_images - true_images

        # False Negatives (FN): Images that should have been retrieved but were not
        fn = true_images - retrieved_images

        # True Negatives (TN): Images that are not in the label and not retrieved
        tn = (all_test_images - true_images) - (retrieved_images - true_images)

        # Accuracy calculation including TN
        label_total_images = len(tp) + len(fp) + len(fn) + len(tn)
        label_accuracy = (len(tp) + len(tn)) / label_total_images if label_total_images > 0 else 0

        # Accumulate for total accuracy calculation
        total_tp += len(tp)
        total_tn += len(tn)
        total_images += label_total_images

        # Store label accuracy
        label_accuracies[label] = label_accuracy

        # Print the results for this label
        print(f"Label: {label}")
        if train_flag:
            print(f"Image Count in Train: {len(train_images)}")
        else:
            print(f"Image Count in Train: 0")
        print(f"Image Count in Test: {len(true_images)}")
        print(f"Retrieved Image Count: {len(retrieved_images)}")
        print(f"Accuracy (with TN): {label_accuracy:.2f}")
        print(f"False Positives (FP): {list(fp)}")
        print(f"False Negatives (FN): {list(fn)}")
        print(f"True Negatives (TN): {len(tn)}")
        print("-" * 30)

    # Calculate total accuracy including TN
    total_accuracy = (total_tp + total_tn) / total_images if total_images > 0 else 0

    # Print total accuracy with color
    print("\033[92m" + f"Total Accuracy (with TN): {total_accuracy:.2f}" + "\033[0m")  # Green text

    # Prepare data for Excel
    row_data = {'Model Name': model_name, 'Test Dataset': test_dataset_name}
    row_data.update(label_accuracies)
    row_data['Total Accuracy'] = total_accuracy

    # Return the row data instead of writing to Excel
    return row_data

model_dir = '/content/drive/My Drive/Image_text models/CLIP_fine-tunned_conservative/'

# Initialize the list to collect all results
all_results = []

# Collect all unique labels across all datasets
all_labels = set()

for test_subdir in test_list_path:
    print(f"Testing {test_subdir}")
    model_name = model_dir.split('/')[-2]
    test_image_path = f'/content/drive/My Drive/Building_images_test_case/{test_subdir}'
    label_to_images, unique_labels = load_test_labels(f'{test_image_path}/test_labels.json')
    model = CLIPModel.from_pretrained(model_dir)
    processor = CLIPProcessor.from_pretrained(model_dir)

    # Call the modified test_result function and collect the results
    row_data = test_result(
        unique_labels=unique_labels,
        label_to_images=label_to_images,
        model=model,
        processor=processor,
        test_image_path=test_image_path,
        model_name=model_name,
        test_dataset_name=test_subdir,
        dynamic_threshold=10
    )
    all_results.append(row_data)  # Add the row data to the list

    # Update the set of all labels
    all_labels.update(unique_labels)
    print("=" * 150)
    print("=" * 150)
    print("=" * 150)

# After processing all datasets, create a DataFrame
df = pd.DataFrame(all_results)

# Ensure all labels are included as columns in the DataFrame
for label in all_labels:
    if label not in df.columns:
        df[label] = None  # Set missing labels to None

# Reorder columns to have 'Model Name', 'Test Dataset', labels, and 'Total Accuracy'
columns_order = ['Model Name', 'Test Dataset'] + sorted(all_labels) + ['Total Accuracy']
df = df.reindex(columns=columns_order)

# Replace NaN values with 'None' as a string
df = df.fillna('None')

# Write the DataFrame to Excel
output_excel_path = f'{model_name}_all_datasets.xlsx'
df.to_excel(output_excel_path, index=False)
print(f"All results written to {output_excel_path}")

"""# Normal with batch 8"""

def search_image_finetuned(model, processor, query, image_dir, threshold=4, fix_threshold=0.23):
    """
    Args:
        query (str): The text query to search for.
        image_dir (str): Directory containing the images.
        threshold (float): The percentage drop threshold between consecutive scores.

    Returns:
        List of tuples (image_name, score) and a list of corresponding images.
    """
    model.eval()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    # Prepare the text input
    text_inputs = processor(text=[query], return_tensors="pt", padding=True)
    text_inputs = {k: v.to(device) for k, v in text_inputs.items()}

    # Get text embedding
    with torch.no_grad():
        text_features = model.get_text_features(**text_inputs)
        text_features /= text_features.norm(dim=-1, keepdim=True)  # Normalize

    # Compute similarity scores for all images
    image_scores = []
    images = {}

    for image_name in os.listdir(image_dir):
        if image_name.endswith(".json"):
            continue
        image_path = os.path.join(image_dir, image_name)
        image = Image.open(image_path).convert("RGB")

        # Process the image
        image_inputs = processor(images=image, return_tensors="pt")
        image_inputs = {k: v.to(device) for k, v in image_inputs.items()}

        # Get image embedding
        with torch.no_grad():
            image_features = model.get_image_features(**image_inputs)
            image_features /= image_features.norm(dim=-1, keepdim=True)  # Normalize

        # Compute cosine similarity
        similarity = torch.matmul(text_features, image_features.T).item()
        image_scores.append((image_name, similarity))
        images[image_name] = image

    # Sort by similarity score in descending order
    image_scores = sorted(image_scores, key=lambda x: x[1], reverse=True)

    # Select images based on score drop
    selected_scores = [image_scores[0]]  # Always include the first image
    for i in range(1, len(image_scores)):
        prev_score = image_scores[i - 1][1]
        curr_score = image_scores[i][1]
        percentage_drop = (prev_score - curr_score) / prev_score
        percentage_drop = percentage_drop * 100

        if percentage_drop > threshold:
            break
        elif curr_score > 0.22:
            selected_scores.append(image_scores[i])


    if image_scores[0][1] < fix_threshold:
        logger.info(f"All the images are bellow threshold, fix_threshold: {fix_threshold}, dynamic threshold: {threshold}")
        selected_scores = []

    # Get the corresponding images
    selected_images = [image_name for image_name, _ in selected_scores]

    return set(selected_images)

import pandas as pd

def test_result(unique_labels, label_to_images, model, processor, test_image_path, model_name, test_dataset_name, dynamic_threshold):
    total_tp = 0
    total_tn = 0
    total_images = 0
    label_accuracies = {}  # To store accuracies for each label
    all_test_images = set.union(*label_to_images.values())  # Union of all test images

    for label in unique_labels:
        # Ground truth: images that should be retrieved for this label
        true_images = label_to_images[label]

        train_flag = 1
        try:
            train_images = label_to_images_train[label]
        except:
            train_flag = 0

        # Retrieved images from the model
        retrieved_images = search_image_finetuned(
            model=model,
            processor=processor,
            query=label,
            image_dir=test_image_path,
            threshold=dynamic_threshold,
            fix_threshold=0.23
        )

        retrieved_images = set(retrieved_images)  # Ensure it's a set for set operations

        # True Positives (TP): Correctly retrieved images
        tp = true_images & retrieved_images

        # False Positives (FP): Images incorrectly retrieved
        fp = retrieved_images - true_images

        # False Negatives (FN): Images that should have been retrieved but were not
        fn = true_images - retrieved_images

        # True Negatives (TN): Images that are not in the label and not retrieved
        tn = (all_test_images - true_images) - (retrieved_images - true_images)

        # Accuracy calculation including TN
        label_total_images = len(tp) + len(fp) + len(fn) + len(tn)
        label_accuracy = (len(tp) + len(tn)) / label_total_images if label_total_images > 0 else 0

        # Accumulate for total accuracy calculation
        total_tp += len(tp)
        total_tn += len(tn)
        total_images += label_total_images

        # Store label accuracy
        label_accuracies[label] = label_accuracy

        # Print the results for this label
        print(f"Label: {label}")
        if train_flag:
            print(f"Image Count in Train: {len(train_images)}")
        else:
            print(f"Image Count in Train: 0")
        print(f"Image Count in Test: {len(true_images)}")
        print(f"Retrieved Image Count: {len(retrieved_images)}")
        print(f"Accuracy (with TN): {label_accuracy:.2f}")
        print(f"False Positives (FP): {list(fp)}")
        print(f"False Negatives (FN): {list(fn)}")
        print(f"True Negatives (TN): {len(tn)}")
        print("-" * 30)

    # Calculate total accuracy including TN
    total_accuracy = (total_tp + total_tn) / total_images if total_images > 0 else 0

    if

    # Print total accuracy with color
    print("\033[92m" + f"Total Accuracy (with TN): {total_accuracy:.2f}" + "\033[0m")  # Green text

    # Prepare data for Excel
    row_data = {'Model Name': model_name, 'Test Dataset': test_dataset_name}
    row_data.update(label_accuracies)
    row_data['Total Accuracy'] = total_accuracy

    # Return the row data instead of writing to Excel
    return row_data

model_dir = '/content/drive/My Drive/Image_text models/CLIP_fine-tunned_batch8/'

# Initialize the list to collect all results
all_results = []

# Collect all unique labels across all datasets
all_labels = set()

for test_subdir in test_list_path:
    print(f"Testing {test_subdir}")
    model_name = model_dir.split('/')[-2]
    test_image_path = f'/content/drive/My Drive/Building_images_test_case/{test_subdir}'
    label_to_images, unique_labels = load_test_labels(f'{test_image_path}/test_labels.json')
    model = CLIPModel.from_pretrained(model_dir)
    processor = CLIPProcessor.from_pretrained(model_dir)

    # Call the modified test_result function and collect the results
    row_data = test_result(
        unique_labels=unique_labels,
        label_to_images=label_to_images,
        model=model,
        processor=processor,
        test_image_path=test_image_path,
        model_name=model_name,
        test_dataset_name=test_subdir,
        dynamic_threshold = 2.5
    )
    all_results.append(row_data)  # Add the row data to the list

    # Update the set of all labels
    all_labels.update(unique_labels)
    print("=" * 150)
    print("=" * 150)
    print("=" * 150)

# After processing all datasets, create a DataFrame
df = pd.DataFrame(all_results)

# Ensure all labels are included as columns in the DataFrame
for label in all_labels:
    if label not in df.columns:
        df[label] = None  # Set missing labels to None

# Reorder columns to have 'Model Name', 'Test Dataset', labels, and 'Total Accuracy'
columns_order = ['Model Name', 'Test Dataset'] + sorted(all_labels) + ['Total Accuracy']
df = df.reindex(columns=columns_order)

# Replace NaN values with 'None' as a string
df = df.fillna('None')

# Write the DataFrame to Excel
output_excel_path = f'{model_name}_all_datasets.xlsx'
df.to_excel(output_excel_path, index=False)
print(f"All results written to {output_excel_path}")

"""# Freezing 1st layer"""

model_dir = '/content/drive/My Drive/Image_text models/CLIP_fine-tunned_freeze_1layer/'

# Initialize the list to collect all results
all_results = []

# Collect all unique labels across all datasets
all_labels = set()

for test_subdir in test_list_path:
    print(f"Testing {test_subdir}")
    model_name = model_dir.split('/')[-2]
    test_image_path = f'/content/drive/My Drive/Building_images_test_case/{test_subdir}'
    label_to_images, unique_labels = load_test_labels(f'{test_image_path}/test_labels.json')
    model = CLIPModel.from_pretrained(model_dir)
    processor = CLIPProcessor.from_pretrained(model_dir)

    # Call the modified test_result function and collect the results
    row_data = test_result(
        unique_labels=unique_labels,
        label_to_images=label_to_images,
        model=model,
        processor=processor,
        test_image_path=test_image_path,
        model_name=model_name,
        test_dataset_name=test_subdir,
        dynamic_threshold=3
    )
    all_results.append(row_data)  # Add the row data to the list

    # Update the set of all labels
    all_labels.update(unique_labels)
    print("=" * 150)
    print("=" * 150)
    print("=" * 150)

# After processing all datasets, create a DataFrame
df = pd.DataFrame(all_results)

# Ensure all labels are included as columns in the DataFrame
for label in all_labels:
    if label not in df.columns:
        df[label] = None  # Set missing labels to None

# Reorder columns to have 'Model Name', 'Test Dataset', labels, and 'Total Accuracy'
columns_order = ['Model Name', 'Test Dataset'] + sorted(all_labels) + ['Total Accuracy']
df = df.reindex(columns=columns_order)

# Replace NaN values with 'None' as a string
df = df.fillna('None')

# Write the DataFrame to Excel
output_excel_path = f'{model_name}_all_datasets.xlsx'
df.to_excel(output_excel_path, index=False)
print(f"All results written to {output_excel_path}")

"""# Freezing 2 layer"""

model_dir = '/content/drive/My Drive/Image_text models/CLIP_fine-tunned_freeze_2layer/'

# Initialize the list to collect all results
all_results = []

# Collect all unique labels across all datasets
all_labels = set()

for test_subdir in test_list_path:
    print(f"Testing {test_subdir}")
    model_name = model_dir.split('/')[-2]
    test_image_path = f'/content/drive/My Drive/Building_images_test_case/{test_subdir}'
    label_to_images, unique_labels = load_test_labels(f'{test_image_path}/test_labels.json')
    model = CLIPModel.from_pretrained(model_dir)
    processor = CLIPProcessor.from_pretrained(model_dir)

    # Call the modified test_result function and collect the results
    row_data = test_result(
        unique_labels=unique_labels,
        label_to_images=label_to_images,
        model=model,
        processor=processor,
        test_image_path=test_image_path,
        model_name=model_name,
        test_dataset_name=test_subdir,
        dynamic_threshold=3
    )
    all_results.append(row_data)  # Add the row data to the list

    # Update the set of all labels
    all_labels.update(unique_labels)
    print("=" * 150)
    print("=" * 150)
    print("=" * 150)

# After processing all datasets, create a DataFrame
df = pd.DataFrame(all_results)

# Ensure all labels are included as columns in the DataFrame
for label in all_labels:
    if label not in df.columns:
        df[label] = None  # Set missing labels to None

# Reorder columns to have 'Model Name', 'Test Dataset', labels, and 'Total Accuracy'
columns_order = ['Model Name', 'Test Dataset'] + sorted(all_labels) + ['Total Accuracy']
df = df.reindex(columns=columns_order)

# Replace NaN values with 'None' as a string
df = df.fillna('None')

# Write the DataFrame to Excel
output_excel_path = f'{model_name}_all_datasets.xlsx'
df.to_excel(output_excel_path, index=False)
print(f"All results written to {output_excel_path}")

"""# Freezing 10 layer"""

model_dir = '/content/drive/My Drive/Image_text models/CLIP_fine-tunned_freeze_10layer/'

# Initialize the list to collect all results
all_results = []

# Collect all unique labels across all datasets
all_labels = set()

for test_subdir in test_list_path:
    print(f"Testing {test_subdir}")
    model_name = model_dir.split('/')[-2]
    test_image_path = f'/content/drive/My Drive/Building_images_test_case/{test_subdir}'
    label_to_images, unique_labels = load_test_labels(f'{test_image_path}/test_labels.json')
    model = CLIPModel.from_pretrained(model_dir)
    processor = CLIPProcessor.from_pretrained(model_dir)

    # Call the modified test_result function and collect the results
    row_data = test_result(
        unique_labels=unique_labels,
        label_to_images=label_to_images,
        model=model,
        processor=processor,
        test_image_path=test_image_path,
        model_name=model_name,
        test_dataset_name=test_subdir,
        dynamic_threshold=3
    )
    all_results.append(row_data)  # Add the row data to the list

    # Update the set of all labels
    all_labels.update(unique_labels)
    print("=" * 150)
    print("=" * 150)
    print("=" * 150)

# After processing all datasets, create a DataFrame
df = pd.DataFrame(all_results)

# Ensure all labels are included as columns in the DataFrame
for label in all_labels:
    if label not in df.columns:
        df[label] = None  # Set missing labels to None

# Reorder columns to have 'Model Name', 'Test Dataset', labels, and 'Total Accuracy'
columns_order = ['Model Name', 'Test Dataset'] + sorted(all_labels) + ['Total Accuracy']
df = df.reindex(columns=columns_order)

# Replace NaN values with 'None' as a string
df = df.fillna('None')

# Write the DataFrame to Excel
output_excel_path = f'{model_name}_all_datasets.xlsx'
df.to_excel(output_excel_path, index=False)
print(f"All results written to {output_excel_path}")

"""# NCEloss"""

model_dir = '/content/drive/My Drive/Image_text models/CLIP_fine-tunned_NCEloss/'

# Initialize the list to collect all results
all_results = []

# Collect all unique labels across all datasets
all_labels = set()

for test_subdir in test_list_path:
    print(f"Testing {test_subdir}")
    model_name = model_dir.split('/')[-2]
    test_image_path = f'/content/drive/My Drive/Building_images_test_case/{test_subdir}'
    label_to_images, unique_labels = load_test_labels(f'{test_image_path}/test_labels.json')
    model = CLIPModel.from_pretrained(model_dir)
    processor = CLIPProcessor.from_pretrained(model_dir)

    # Call the modified test_result function and collect the results
    row_data = test_result(
        unique_labels=unique_labels,
        label_to_images=label_to_images,
        model=model,
        processor=processor,
        test_image_path=test_image_path,
        model_name=model_name,
        test_dataset_name=test_subdir,
        dynamic_threshold=3
    )
    all_results.append(row_data)  # Add the row data to the list

    # Update the set of all labels
    all_labels.update(unique_labels)
    print("=" * 150)
    print("=" * 150)
    print("=" * 150)

# After processing all datasets, create a DataFrame
df = pd.DataFrame(all_results)

# Ensure all labels are included as columns in the DataFrame
for label in all_labels:
    if label not in df.columns:
        df[label] = None  # Set missing labels to None

# Reorder columns to have 'Model Name', 'Test Dataset', labels, and 'Total Accuracy'
columns_order = ['Model Name', 'Test Dataset'] + sorted(all_labels) + ['Total Accuracy']
df = df.reindex(columns=columns_order)

# Replace NaN values with 'None' as a string
df = df.fillna('None')

# Write the DataFrame to Excel
output_excel_path = f'{model_name}_all_datasets.xlsx'
df.to_excel(output_excel_path, index=False)
print(f"All results written to {output_excel_path}")

"""# Normal with batch 2"""

model_dir = '/content/drive/My Drive/Image_text models/CLIP_fine-tunned_batch2/'

# Initialize the list to collect all results
all_results = []

# Collect all unique labels across all datasets
all_labels = set()

for test_subdir in test_list_path:
    print(f"Testing {test_subdir}")
    model_name = model_dir.split('/')[-2]
    test_image_path = f'/content/drive/My Drive/Building_images_test_case/{test_subdir}'
    label_to_images, unique_labels = load_test_labels(f'{test_image_path}/test_labels.json')
    model = CLIPModel.from_pretrained(model_dir)
    processor = CLIPProcessor.from_pretrained(model_dir)

    # Call the modified test_result function and collect the results
    row_data = test_result(
        unique_labels=unique_labels,
        label_to_images=label_to_images,
        model=model,
        processor=processor,
        test_image_path=test_image_path,
        model_name=model_name,
        test_dataset_name=test_subdir,
        dynamic_threshold=4
    )
    all_results.append(row_data)  # Add the row data to the list

    # Update the set of all labels
    all_labels.update(unique_labels)
    print("=" * 150)
    print("=" * 150)
    print("=" * 150)

# After processing all datasets, create a DataFrame
df = pd.DataFrame(all_results)

# Ensure all labels are included as columns in the DataFrame
for label in all_labels:
    if label not in df.columns:
        df[label] = None  # Set missing labels to None

# Reorder columns to have 'Model Name', 'Test Dataset', labels, and 'Total Accuracy'
columns_order = ['Model Name', 'Test Dataset'] + sorted(all_labels) + ['Total Accuracy']
df = df.reindex(columns=columns_order)

# Replace NaN values with 'None' as a string
df = df.fillna('None')

# Write the DataFrame to Excel
output_excel_path = f'{model_name}_all_datasets.xlsx'
df.to_excel(output_excel_path, index=False)
print(f"All results written to {output_excel_path}")

"""# Normal with batch 16"""

model_dir = '/content/drive/My Drive/Image_text models/CLIP_fine-tunned_batch16/'

# Initialize the list to collect all results
all_results = []

# Collect all unique labels across all datasets
all_labels = set()

for test_subdir in test_list_path:
    print(f"Testing {test_subdir}")
    model_name = model_dir.split('/')[-2]
    test_image_path = f'/content/drive/My Drive/Building_images_test_case/{test_subdir}'
    label_to_images, unique_labels = load_test_labels(f'{test_image_path}/test_labels.json')
    model = CLIPModel.from_pretrained(model_dir)
    processor = CLIPProcessor.from_pretrained(model_dir)

    # Call the modified test_result function and collect the results
    row_data = test_result(
        unique_labels=unique_labels,
        label_to_images=label_to_images,
        model=model,
        processor=processor,
        test_image_path=test_image_path,
        model_name=model_name,
        test_dataset_name=test_subdir,
        dynamic_threshold=4
    )
    all_results.append(row_data)  # Add the row data to the list

    # Update the set of all labels
    all_labels.update(unique_labels)
    print("=" * 150)
    print("=" * 150)
    print("=" * 150)

# After processing all datasets, create a DataFrame
df = pd.DataFrame(all_results)

# Ensure all labels are included as columns in the DataFrame
for label in all_labels:
    if label not in df.columns:
        df[label] = None  # Set missing labels to None

# Reorder columns to have 'Model Name', 'Test Dataset', labels, and 'Total Accuracy'
columns_order = ['Model Name', 'Test Dataset'] + sorted(all_labels) + ['Total Accuracy']
df = df.reindex(columns=columns_order)

# Replace NaN values with 'None' as a string
df = df.fillna('None')

# Write the DataFrame to Excel
output_excel_path = f'{model_name}_all_datasets.xlsx'
df.to_excel(output_excel_path, index=False)
print(f"All results written to {output_excel_path}")

